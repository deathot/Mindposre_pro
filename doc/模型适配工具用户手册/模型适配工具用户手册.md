# 模型适配工具用户手册

# 版本

1.1.1

# 1 简介

## 1.1 工具简介

   模型适配工具是一款集成数据标注、模型迁移学习、模型打包为一体的开发者工具套件。通过此工具套件，可以降低开发者在模型开发过程中对AI专业知识/深度学习框架学习的要求，极大降低开发的难度、复杂度。

**模型适配工具特性：**

- 训练更简单：0代码开发，提供可视化界面完成数据集制作，模型迁移学习等操作。
- 模型场景多元：支持图像分类、目标检测跟踪、姿态关键点检测、图像分割。
- 模型高精准：借助内置预训练模型，迁移学习，减少学习时间，提高模型准确性。
- 模型易部署：支持将训练的模型，转换打包为压缩包，用户可传输到200I DK A2上直接运行，无需面对模型转换失败、部署困难的问题。

## 1.2 系统要求

| 配置类型 | 推荐配置       |
| -------- | -------------- |
| 系统类型 | windows 10、11 |
| CPU      | 8 core CPU     |
| 内存RAM  | 32GB           |
| 磁盘     | 100G SSD存储   |
|          |                |

# 2 模型适配工具的环境准备与运行

## 2.1 anaconda的安装

1. 安装Anaconda3
   到Anaconda官网 https://www.anaconda.com/ 下载Windows版本的Anaconda3，并安装。这里下载的是Anaconda3-2022.05-Windows-x86_64.exe。
   运行conda安装软件，直接点击"next"
   ![输入图片说明](https://foruda.gitee.com/images/1668394271671697100/8795236f_1925050.png "屏幕截图")
   选择"IAgree"
   ![输入图片说明](https://foruda.gitee.com/images/1668394305079649092/23eea088_1925050.png "屏幕截图")
   选择"Just Me"
   ![输入图片说明](https://foruda.gitee.com/images/1668394365311535438/11283732_1925050.png "屏幕截图")
   选择安装路径，也可以默认安装路径，路径中不能含有中文
   ![输入图片说明](https://foruda.gitee.com/images/1668394470026767709/566e5638_1925050.png "屏幕截图")
   两个选择框都勾选上，这样软件会自动添加环境变量，然后点击"Install"
   ![输入图片说明](https://foruda.gitee.com/images/1668394618619081463/a3ca7270_1925050.png "屏幕截图")
   等待安装完成
   ![输入图片说明](https://foruda.gitee.com/images/1668394653693559942/1ad09940_1925050.png "屏幕截图")
   点击"next"
   ![输入图片说明](https://foruda.gitee.com/images/1668395131580580347/3395f2a7_1925050.png "屏幕截图")
   点击"finsh",安装完成
   ![输入图片说明](https://foruda.gitee.com/images/1668395255404739451/046f7e1a_1925050.png "屏幕截图")
   验证conda安装成功,打开cmd执行 conda,不报错就成功

   ``` bash
    conda
   ```

   ![输入图片说明](https://foruda.gitee.com/images/1668394899970513288/ef64b003_1925050.png "屏幕截图")

## 2.2 模型适配工具conda虚拟环境创建

模型适配工具使用的conda环境名固定为：model-adapter-tool
当前模型训练使用的python版本为：3.8.13

### 2.2.1 创建虚拟环境

1. 下载conda环境的虚拟环境的压缩包。
   通过[下载地址](https://ascend-devkit-tool.obs.cn-south-1.myhuaweicloud.com/model-adapter-tool-env/1.0.7/model-adapter-tool.tar.gz)下载conda的虚拟环境包。
    ![输入图片说明](https://foruda.gitee.com/images/1668391489220966705/4adf8b95_1925050.png "屏幕截图")

2. 解压下载的压缩包到conda的虚拟环境中：
   查找自己conda环境安装目录, 通过conda info --env 查找conda的安装环境

   ``` base
   conda info --env
   ```

   ![输入图片说明](https://foruda.gitee.com/images/1668391956424018597/67015f0e_1925050.png "屏幕截图")
   进入到conda的安装目录，找到envs文件夹，并在该目录下新建一个 "model-adapter-tool" 文件夹
   ![输入图片说明](https://foruda.gitee.com/images/1668392161590407783/cb5a1ed5_1925050.png "屏幕截图")
   使用解压软件将第一步下载好的压缩包解压到建好的"model-adapter-tool"文件夹中
   ![输入图片说明](https://foruda.gitee.com/images/1668392359877574987/e021dcc5_1925050.png "屏幕截图")
   遇到弹窗，选择“全部选是”
![输入图片说明](https://foruda.gitee.com/images/1679534895010550748/f749f0a2_11225962.png "屏幕截图")

3. 查看已经创建的虚拟环境，如果出现"model-adapter-tool"的环境，就说明虚拟环境安装成功

   ```
   conda info --env
   ```

   ![输入图片说明](https://foruda.gitee.com/images/1668392888227632098/a24a8558_1925050.png "屏幕截图")


# 3 模型适配工具的安装与卸载

## 3.1 模型适配工具的安装

**1.双击模型适配工具安装包“Ascend-devkit_model-adapter_xxx_win10-x86_64.exe”，或者右键选择打开。**

![输入图片说明](https://foruda.gitee.com/images/1679882822045777124/550dc700_11225962.png "屏幕截图")

**2.进入“Windows 已保护你的电脑“界面，选择”更多信息“**

![输入图片说明](https://foruda.gitee.com/images/1679882859301977671/908e1a75_11225962.png "屏幕截图")

**3.出现”仍要运行“选择，选择”仍要运行“**

![输入图片说明](https://foruda.gitee.com/images/1679882882261025059/3a78f790_11225962.png "屏幕截图")

**4. 进入“用户账户控制”界面，选择“是”。**

![输入图片说明](https://foruda.gitee.com/images/1679882906029718926/587b1bbb_11225962.png "屏幕截图")

**5.进入模型适配工具安装界面，选择“next”，进入下一步。**

![输入图片说明](https://foruda.gitee.com/images/1679883059854730733/c2322776_11225962.png "屏幕截图")

**6.进入选择安装路径界面，默认路径为：C:\Program Files (x86)\Ascend-devkit_model-adapter_xxx_win10-x86_64， 确认安装路径后，选择“Install”，进入下一步。**

![输入图片说明](https://foruda.gitee.com/images/1679883142145375226/ac10a688_11225962.png "屏幕截图")

**7.等待安装完成后，选择“Finish”完成安装。默认勾选“Run Ascend-devkit model-adapter xxx”，即完成后自动启动模型适配工具。**

![输入图片说明](https://foruda.gitee.com/images/1679883177475194503/78e0e079_11225962.png "屏幕截图")

## 3.2 模型适配工具的卸载

**1.点击windows，右键选中模型适配工具，选择卸载。或者直接进入windows的程序和功能页面。**

![输入图片说明](https://foruda.gitee.com/images/1673925632595671485/a0f3bc98_11225962.png "屏幕截图")

**2.进入windows程序和功能页面，右键选择模型适配工具，选择“卸载/更改”**

![输入图片说明](https://foruda.gitee.com/images/1679883251643738034/2eccdb20_11225962.png "屏幕截图")

**3.进入“用户账户控制”界面，选择“是”**

![输入图片说明](https://foruda.gitee.com/images/1673925648314333225/e75f1360_11225962.png "屏幕截图")

**4.进入确认是否卸载界面，选择“是”，等待卸载完成**

**注：卸载会删除配置的环境变量“model-adapter-tool”**

![输入图片说明](https://foruda.gitee.com/images/1679883276119860593/50fcff7f_11225962.png "屏幕截图")

**5.选择“确定”，完成卸载**

![输入图片说明](https://foruda.gitee.com/images/1679883290023474196/dbd2bfc5_11225962.png "屏幕截图")

## 3.3 模型适配工具的更新

**1.打开软件，如果检测到有更新，则会弹窗提示。选择"确定"则进行更新下载。选择"取消"或者关闭窗口，则继续使用旧版本。**

![输入图片说明](https://foruda.gitee.com/images/1679535376011953179/037983f7_11225962.png "屏幕截图")

**2.点击"确定", 开始进行下载升级包。**

![输入图片说明](https://foruda.gitee.com/images/1678361493446008640/f468265c_11225962.png "屏幕截图")

**3.下载完成后，弹窗消失，等待片刻，则会自动安装新版本。**

![输入图片说明](https://foruda.gitee.com/images/1678361551320802276/9530be1d_11225962.png "屏幕截图")

**4.安装完成后，点击Finish即完成升级。**

![输入图片说明](https://foruda.gitee.com/images/1679883330383856950/dd4d57a7_11225962.png "屏幕截图")


## 3.4 模型适配工具运行

1. 点击运行软件包`Ascend-devkit_model-adapter_x.x.x_win10-x86_64.exe`,显示界面如图，点击“开始”后对conda环境进行检测：
   ![输入图片说明](https://foruda.gitee.com/images/1679534152240798479/1c28b4ff_11225962.png "屏幕截图")
   如果环境未配置好，则会提示：
   ![输入图片说明](https://foruda.gitee.com/images/1679534181108057213/ab13fd56_11225962.png "屏幕截图")
   此时可根据安装指引进行环境的安装，再点击重新检测即可。

2. 在模型运行界面，根据需要选择模型算法，当前支持分类、检测、分割和关键点检测4类模型的任务。
   ![输入图片说明](https://foruda.gitee.com/images/1678093606161835132/93e3be7b_11225962.png "屏幕截图")

3. 选择模型任务，进入标注界面，快捷键如下，以检测模型为例：

Ctrl + O : 打开图片
D ：切换下一张图片
A : 切换上一张图片
Ctrl + U : 打开目录
Ctrl + S : 保存标注
Ctrl + W : 关闭图片，保存已有标签
Ctrl + Del : 永久删除标签文件
Ctrl + Q : 退出
Ctrl + E : 编辑标注
Ctrl + C : 复制标注
Ctrl + V ：粘贴标注
Del : 删除选择标注
Ctrl + Z : 撤销/撤销最后控制点
Ctrl + = : 放大
Ctrl + - : 缩小
Ctrl + 0 : 原始大小
Ctrl + F : 适应窗口 
Ctrl + Shift + F : 适应宽度


![输入图片说明](https://foruda.gitee.com/images/1679564415423971506/fff78490_11225962.png "屏幕截图")


# 4 模型适配操作

## 4.1 目标检测跟踪算法Yolov5-Deepsort

### 4.1.1 快速使用

烧录的镜像会内置sample样例，可在200I DK A2上调用以下命令运行：

```
cd /home/HwHiAiUser/samples/model-adapter-models/det/infer_project/
python3 om_infer.py --model=output/yolov5s_bs1.om --eval --visible
```

运行成功后，会输出检测结果，并保存在./output/img路径下，效果图如下图所示：

![输入图片说明](https://foruda.gitee.com/images/1679536837854920266/dc557e8c_11225962.png "屏幕截图")

### 4.1.2 自定义数据集

1.**数据收集**：

- 收集包含待分类目标的png、jpg、JPEG、bmp、webp格式图片数据备用，推荐使用jpg格式。

- 图片要求：图片分辨率不高于1080P，推荐单张图片小于1M，每个类别的图片数量在100张左右。

2.**图像标注**：

 打开模型适配工具，选择检测模型，进入标注界面。单击"打开目录", 选择图片存放文件夹目录。如下图。
	
 labelme界面介绍：
	
（1）读入图片；	
（2）当前图片所有标注；	
（3）当前图片的标注信息；	
（4）所有图片的路径，点击可以切换到对应的图片；	
（5）鼠标移动到“绘制矩形”快捷键，开始进行矩形框标注。	
注：标注时输入标签仅支持数字、字母和下划线。

![输入图片说明](https://foruda.gitee.com/images/1679537323428232276/2929c00e_11225962.png "屏幕截图")

   标注完后的图片信息，文件列表一栏，所有标注操作完的图片会自动打勾。

   标注完成后，会在图片所在同级目录下，生成记录有标注信息的json文件。

<div align="center">
    	<img src="https://foruda.gitee.com/images/1661847566222345469/c7084a0b_11420827.png" style="zoom:50%;"/>  
    </a>
</div>
<div align = "center">图片3</div>

自定义数据集说明:

- 图像的多样性：数据集图片要从实际模型部署使用的环境获得；
- 标注完整性：要将图片中的所有待检测目标都标注出来，任何目标都不能缺少标签。漏标注将影响模型精度；
- 标注准确性：边框需要紧密框住每个目标，且类别正确，标注无误。


### 4.1.3 模型一键迁移

1.**一键迁移工具入口**

 点击“一键迁移”调用迁移

2.**配置参数**

- 数据集路径：选择标注好的数据集路径；(**注：图片数量至少需要20张**)

- 数据集拆分：拆分训练集与验证集比例，默认0.3；（注：默认拆分10%的测试集用于边缘推理，训练集与验证集按输入拆分比例再次进行拆分。）

- 模型训练：
  第一个参数是训练轮数，推荐100个epoch，
  第二个参数是一次训练所取的样本数，默认12，使用用户允许的最大硬件资源。

- 预训练模型：
  可选yolov5s，yolov5n，yolov5l，yolov5x。默认yolov5s。
  其中yolov5s，yolov5n，yolov5l，yolov5x 按照其所含的残差结构的个数依次增多，网络的特征提取、融合能力不断加强，检测精度得到提高，但相应的时间花费也在增加。以下表格给出不同大小的模型及其结果对比：

| Model   | size(pixels) | mAP 0.5:0.95 | mAP 0.5 | Speed CPU b1 | Speed V100 b1 | Speed V100 b32 | Params |
|---------|--------------|--------------|---------|--------------|---------------|----------------|--------|
| yolov5n | 640          | 28.0         | 45.7    | 45           | 6.3           | 0.6            | 1.9    |
| yolov5s | 640          | 37.4         | 56.8    | 98           | 6.4           | 0.9            | 7.2    |
| yolov5l | 640          | 49.0         | 67.3    | 430          | 10.1          | 2.7            | 46.5   |
| yolov5x | 640          | 50.7         | 68.9    | 766          | 12.1          | 4.8            | 86.7   |

- 输出目录：模型输出路径

  迁移时间与cpu型号、内存、数据集大小有关。

![输入图片说明](https://foruda.gitee.com/images/1679882204639246639/3dee7669_11225962.png "屏幕截图")

**注：特殊字符包含~!@#$%^&+,.、[]！￥……（）——{}：' “，。《》？【】` ·**

3.**输出文件**

![输入图片说明](https://foruda.gitee.com/images/1672918223015204151/d3707433_11225962.png "屏幕截图")

- trans_output:数据集转换生成的中间文件
- train_output:训练生成的中间文件
- 压缩文件：该压缩包中有200I DK A2上推理所需脚本，包括atc转换脚本、acl接口代码、模型推理代码等；

### 4.1.4 边缘推理

（1）把压缩包上传到200I DK A2，进入目录执行解压操作

```
tar -zxvf infer_project.tar.gz
cd infer_project/ && dos2unix `find .`
```

（2）执行模型转换脚本，在输出目录output/下得到om模型yolov5s_bs1.om
```
python3 onnx2om.py
```
（3）执行推理

```python
python3 om_infer.py --model=output/yolov5s_bs1.om --eval --visible
```

## 4.2 图像分类算法MobilenetV3

### 4.2.1 快速使用

烧录的镜像会内置sample样例，可在200I DK A2上调用以下命令运行：

```
cd /home/HwHiAiUser/samples/model-adapter-models/cls/edge_infer/
bash run.sh
```

运行成功后，会输出图像分类结果，会在当前目录下生成cls_output.txt文件，记录每张图片的名称及其分类结果。

### 4.2.2 自定义数据集

1. **数据收集**

   - 收集包含待分类目标的png、jpg、JPEG、bmp、webp格式图片数据备用，推荐使用jpg格式。
   - 图片要求：图片分辨率不高于1080P，推荐单张图片小于1M，每个类别的图片数量在100张左右。

2. **图像标注**

   - 打开模型适配工具，选择分类模型，进入标注界面。

   （1）选择“制作数据集”：

     配置输入数据集地址：图片文件夹地址；

     配置分类类别信息：需要分类的类别名称（名称英文并用逗号隔开，不支持特殊字符），比如：“dog,cat”。

![输入图片说明](https://foruda.gitee.com/images/1672918513900163632/1f2af487_11225962.png "屏幕截图")

 配置完点击“确认”，开始标注，界面如下，对应每张图片，从右侧标记栏选择对应的标签名称，当前图片标注完成后，从左侧文件列表选择下一张图片，直到完成所有图片的标注任务。
	
 注：分类模型下不能在标注界面增加分类类别，如果分类类型有增加，需要重新制作数据集，在制作数据集弹框输入即可。

![输入图片说明](https://foruda.gitee.com/images/1678093726252184938/e0765fe7_11225962.png "屏幕截图")


 标注输出：
     1.在输入图片文件夹地址下，生成与图片一一对应的json文件，记录标注信息；
![输入图片说明](https://foruda.gitee.com/images/1679990596303450303/f67e0178_11225962.png "屏幕截图")
     2.在输入图片文件夹同级目录生成flags.txt文件。
![输入图片说明](https://foruda.gitee.com/images/1679990671768271035/aaae4319_11225962.png "屏幕截图")
    3.flags.txt内容如下所示：
![输入图片说明](https://foruda.gitee.com/images/1679990680592783884/9995f52a_11225962.png "屏幕截图")

（2）选择“已有数据集”：无需进行图像标注，点击“确认”后，可直接开始4.2.3。

### 4.2.3 模型一键迁移

1. **点击“一键迁移”调用迁移**

点击“一键迁移”调用迁移

2. **参数配置解析**

- 数据集路径：选择标注好的数据集路径；(**注：图片数量至少需要20张**)

- 数据集拆分：拆分训练集与验证集比例，默认0.3；（注：默认拆分10%的测试集用于边缘推理，训练集与验证集按输入拆分比例再次进行拆分。）

- 模型训练：
  第一个参数是训练轮数，推荐100个epoch，
  第二个参数是一次训练所取的样本数，默认12，使用用户允许的最大硬件资源。

- 输出目录：模型输出路径

  迁移时间与cpu型号、内存、数据集大小有关。

![输入图片说明](https://foruda.gitee.com/images/1679881946350173722/512fd15f_11225962.png "屏幕截图")

**注：特殊字符包含~!@#$%^&+,.、[]！￥……（）——{}：' “，。《》？【】` ·**

3. **输出文件**

![输入图片说明](https://foruda.gitee.com/images/1672918885186799471/f6299d49_11225962.png "屏幕截图")

- trans_output:数据集转换生成的中间文件
- train_output:训练生成的中间文件
- 压缩文件：该压缩包中有200I DK A2上推理所需脚本，包括atc转换脚本、acl接口代码、模型推理代码等；

### 4.2.4 边缘推理

（1）把压缩包上传到200I DK A2，进入目录执行解压操作

```
tar -xvf edge_infer.tar
cd edge_infer/ && dos2unix `find .`
```

（2）进入解压缩目录执行如下命令,得到om模型

```
bash atc.sh
```

（3）执行推理

```python
bash run.sh
```

（4）精度
推理结果中显示Acc即为精度。


## 4.3 关键点检测跟踪算法AlphaPose-ORB

### 4.3.1 快速使用

烧录的镜像会内置sample样例，可在200I DK A2上调用以下命令运行：

```
cd /home/HwHiAiUser/samples/model-adapter-models/keypoint/infer/
bash build.sh
bash run.sh image ./data/test
```
运行成功后，会输出检测结果，并保存在./out目录下，效果图如下图所示：

![输入图片说明](https://foruda.gitee.com/images/1679538696868840674/1f6a3e29_11225962.png "屏幕截图")

### 4.3.2 自定义数据集

1. **数据收集**

- 收集person(人)类别的png、jpg、JPEG、bmp、webp格式图片数据备用，推荐使用jpg格式。
- 图片要求：图片分辨率不高于1080P，推荐单张图片小于1M，图片数量在100张左右。

2. **图像标注**

- 打开模型适配工具，进入标注界面。单击"打开目录", 选择图片存放文件夹目录，如下图介绍：

（1）读入图片；

（2）所有图片的路径，点击可以切换到对应的图片；

（3）当前图片标注信息；

（4）鼠标移动到“绘制多边形”快捷键，开始进行人体区域标注，标签名称为person(Group ID根据多人分别设置0,1,2...)。

（5）鼠标移动到“绘制控制点”快捷键，开始进行人体关键点标注，标签名称可选择【 nose, left_eye, right_eye, left_ear, right_ear, left_shoulder, right_shoulder, left_elbow, right_elbow, left_wrist, right_wrist, left_hip, right_hip, left_knee, right_knee, left_ankle, right_ankle 】。Group ID根据多人多边形区域对应一致。

注：标注时输入标签仅支持数字、字母和下划线。

![输入图片说明](https://foruda.gitee.com/images/1679990017009048635/9128ba0f_11225962.png "屏幕截图")

标注完后的图片信息，文件列表一栏，所有标注操作完的图片会自动打勾。

标注完成后，会在图片所在同级目录下，生成记录有标注信息的json文件。

<div align="center">
    	<img src="https://foruda.gitee.com/images/1663725275345512878/9e9e0bdd_9461593.jpeg" style="zoom:60%;">  
    </a>
</div>

自定义数据集说明

- 图像多样性：数据集图片要从实际模型部署使用的环境获得；
- 标注完整性：要将图片中所涉及人的区域、关键点都标注出来，标注不准确或标注错误将影响模型精度；
- 标注准确性：多边形区域需要紧密包围人体，关键点需要定位清楚，标注无误。

### 4.3.3 模型一键迁移

1.**一键迁移工具入口**

 点击“一键迁移”调用迁移工具

2.**配置参数**

参数配置解析：

- 数据集路径：选择标注好的数据集路径；(**注：图片数量至少需要20张**)

- 数据集拆分：拆分训练集与验证集比例，默认0.3；（注：默认拆分10%的测试集用于边缘推理，训练集与验证集按输入拆分比例再次进行拆分。）

- 模型训练：
  第一个参数是训练轮数，推荐100个epoch，
  第二个参数是一次训练所取的样本数，默认12，使用用户允许的最大硬件资源。

- 输出目录：模型输出路径

  迁移时间与cpu型号、内存、数据集大小有关。

![输入图片说明](https://foruda.gitee.com/images/1679882361595892348/8135f7d1_11225962.png "屏幕截图")

**注：特殊字符包含~!@#$%^&+,.、[]！￥……（）——{}：' “，。《》？【】` ·**

3. **输出目录**

![输入图片说明](https://foruda.gitee.com/images/1672918885186799471/f6299d49_11225962.png "屏幕截图")

- trans_output:数据集转换生成的中间文件
- train_output:训练生成的中间文件
- 压缩文件：该压缩包中有200I DK A2上推理所需脚本，包括atc转换脚本、acl接口代码、模型推理代码等；

### 4.3.4 边缘推理

（1）把压缩包上传到200I DK A2，进入目录执行解压操作，并转换文件格式。

```
tar -xvf edge_infer.tar
cd infer/ && dos2unix `find .`
```

（2）在编译运行项目前，需要设置环境变量： 

```
# MindX SDK环境变量、CANN环境变量：
. ${SDK-path}/set_env.sh
. ${ascend-toolkit-path}/set_env.sh
# 示例：
. /usr/local/Ascend/ascend-toolkit/set_env.sh
. /usr/local/Ascend/mxVision/set_env.sh
```

（3）进入解压缩目录，执行命令转换yolov3模型、 AlphaPose模型。

```
cd models/
```

```
atc --model=./yolov3_tf.pb --framework=3 --output=./yolov3_tf_bs1_fp16 --soc_version=Ascend310B1 --insert_op_conf=./aipp_yolov3_416_416.aippconfig --input_shape="input:1,416,416,3" --out_nodes="yolov3/yolov3_head/Conv_22/BiasAdd:0;yolov3/yolov3_head/Conv_14/BiasAdd:0;yolov3/yolov3_head/Conv_6/BiasAdd:0"
```

```
atc --framework=5 --model=fast_res50_256x192_bs1.onnx --output=fast_res50_256x192_aipp_rgb --input_format=NCHW --input_shape="image:1,3,256,192" --soc_version=Ascend310B1 --insert_op_conf=aipp_192_256_rgb.cfg
```

（4）编译

```
cd ../
bash build.sh
```

（5）执行推理，有图片推理、精度测试两种方式，进入infer目录执行：

**运行图片推理**

```
bash run.sh image [INPUT]
# 示例
bash run.sh image ./data/test
```

参数说明：[INPUT]——需要推理的图片目录。

推理结果：命令执行成功后会在infer/out目录下生成以测试图片名称命名的 json 文件，该文件包含图像中人物的关键点位置与置信度信息。查看文件验证人体关键点检测结果。

**运行精度测试**

```
bash run.sh evaluate [INPUT]
# 示例
bash run.sh evaluate ./data
```

参数说明：[INPUT]——需要测试精度的数据集目录，包括验证集和标签压缩文件 。

```
.
├── data
│   ├── annotations
│   │   └── person_keypoints_val2017.json
│   └── val2017
│       ├── 000000581615.jpg
│       ├── 000000581781.jpg
│       └── other-images
```

推理结果： 命令执行结束后输出评测结果，并生成 val2017_keypoint_detect_result.json检测结果文件。 


## 4.4 图像分割算法 Unet++

### 4.4.1 快速使用

烧录的镜像会内置sample样例，可在200I DK A2上调用以下命令运行：

```
cd /home/HwHiAiUser/samples/model-adapter-models/seg/edge_infer/infer/sdk/
bash build.sh
```
运行成功后，会在./infer_result/目录下生成推理结果图片，效果图如下图所示：

![输入图片说明](https://foruda.gitee.com/images/1679538984734637277/c2d4c3f1_11225962.png "屏幕截图")

### 4.4.2 自定义数据集

1. **数据收集**

- 收集包含待分类目标的png、jpg、JPEG、bmp、webp格式图片数据备用，注意目前数据集仅支持包含一种格式（例如，只包含png，或只包含jpg）
- 图片要求：图片分辨率不高于1080P，推荐单张图片小于1M，每个类别的图片数量在100张左右。

2. **图像标注**

- 打开模型适配工具，选择分割模型。进入标注界面。单击"打开目录", 选择图片存放文件夹目录，如下图介绍：

（1）读入图片；

（2）所有图片的路径，点击可以切换到对应的图片；

（3）点击“绘制多边形”，开始进行区域标注；

（4）标注多边形后，输入其类别标签，也可在下方已有类别中选择标签；

（5）标注后，包含标注信息的json文件会默认存入当前文件夹，每个图片对应一个 json 标注文件，同时在（2）处会将已标注的图片打勾。

   注：标注时输入标签仅支持数字、字母和下划线。

![输入图片说明](https://foruda.gitee.com/images/1678094262304673732/77bbdb95_11225962.png "屏幕截图")


标注完成后，会在图片所在同级目录下，生成记录有标注信息的json文件。

![输入图片说明](https://foruda.gitee.com/images/1675230422881938861/71bf6659_11359706.png "image-20230201110907213.png")


自定义数据集说明：

- 图像的多样性：数据集图片要从实际模型部署使用的环境获得；
- 标注完整性：要将图片中的所有待检测目标都标注出来，任何目标都不能缺少标签；
- 标注准确性：多边形需要紧密围绕每个目标，且类别正确，标注无误。

### 4.4.3 模型一键迁移

1.**迁移工具入口**

 点击“一键迁移”调用迁移工具

2.**配置参数**

- 数据集路径：选择标注好的数据集路径；(**注：图片数量至少需要20张**)

- 数据集拆分：拆分训练集与验证集比例，默认0.3；（注：默认拆分10%的测试集用于边缘推理，训练集与验证集按输入拆分比例再次进行拆分。）

- 模型训练：
  第一个参数是训练轮数，推荐 100 个epoch。
  第二个参数是一次训练所取的样本数，默认12，使用用户允许的最大硬件资源。

- 输出目录：模型输出路径

  迁移时间与cpu型号、内存、数据集大小有关。

![输入图片说明](https://foruda.gitee.com/images/1679882315249468785/cb623e6a_11225962.png "屏幕截图")

**注：特殊字符包含~!@#$%^&+,.、[]！￥……（）——{}：' “，。《》？【】` ·**

3. **输出文件**

![输入图片说明](https://foruda.gitee.com/images/1675230440781762965/d47c010a_11359706.png "image-20230201105416029.png")

- trans_output:数据集转换生成的中间文件
- train_output:训练生成的中间文件
- 压缩文件：该压缩包中有200I DK A2上推理所需脚本，包括sdk推理代码、测试数据、所需onnx模型等；

### 4.4.4 边缘推理

（1）把压缩包上传到200I DK A2，进入目录执行解压操作。

```
tar -xvf edge_infer.tar
cd edge_infer/ && dos2unix `find .`
```

（2）在编译运行项目前，需要设置环境变量： 

```
# MindX SDK环境变量、CANN环境变量：
. ${SDK-path}/set_env.sh
. ${ascend-toolkit-path}/set_env.sh
# 示例：
. /usr/local/Ascend/ascend-toolkit/set_env.sh
. /usr/local/Ascend/mxVision/set_env.sh
```

（3）进入解压缩目录，执行命令转换 unet++ 模型。

```
# 进入解压缩目录
cd edge_infer
```

```
# om 模型转换命令
atc --framework=5 [--model] [--input_format] [--input_shape] [--output] --log=error --soc_version=Ascend310B1
# 示例：
atc --framework=5 --model=./model.onnx --input_format=NCHW --input_shape="actual_input_1:1,3,96,96" --output=model_bs1 --log=error --soc_version=Ascend310B1
```

参数说明：

[--model] —— 解压文件夹中的onnx模型

[--input_format] —— 输入数据格式

[--input_shape]——模型的输入节点名称和形状

[--output] —— 输出om模型名字

（4）执行推理，有图片文件夹推理、视频推理两种方式，进入 edge_infer/infer/sdk 目录执行：

**图片文件夹推理**

```
# 进入推理目录
cd infer/sdk
# 执行推理脚本
bash build.sh
```

推理结果：命令执行成功后会在当前目录下生成 infer_result 文件夹，可查看推理后图片结果。

**视频推理**

用户可自行准备视频，命名为 video.mp4，也可点击 [链接](https://ascend-devkit-tool.obs.cn-south-1.myhuaweicloud.com/pre-training_model/segmentation/video.mp4) 下载示例小方块视频，上传到 edge_infer/sample_data 目录，并执行以下命令

```
# 进入推理目录
cd infer/sdk
# 执行推理脚本
bash build_video.sh
```

推理结果：推理后视频结果保存在 infer_result/video.mp4。

# 5. FAQ

## 5.1 使用模型适配工具一键迁移时，断电、重启、进程挂死导致输出文件夹残留。

当使用模型适配工具一键迁移时，出现断电、重启、进程挂死等问题，导致一键迁移未完成，输出文件夹残留，需用户自行删除。

## 5.2 模型适配工具界面显示异常。

推荐使用分辨率1920x1080的屏幕。如果屏幕分辨率较大，请将缩放与布局缩小。(右键桌面->显示设置->更改文本、应用等项目的大小)

## 5.3 手动取消迁移，导致模型适配工具异常退出。

当迁移任务正在进行时，此时点击“取消迁移”，则可能会导致工具异常退出，进一步导致输出目录有残留文件，需用户自行删除。