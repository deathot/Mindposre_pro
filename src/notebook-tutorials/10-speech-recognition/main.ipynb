{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047f7d21",
   "metadata": {},
   "source": [
    "# 样例介绍\n",
    "* 自动语音识别，即ASR，指借助计算机将语音转换为文本。在这个样例中，我们使用了基于深度学习的语音识别模型WeNet，借助我们的昇腾Atlas 200I DK A2，可以进行高性能推理。\n",
    "\n",
    "# 前期准备\n",
    "* 基础镜像的样例目录中已包含转换后的om模型以及测试文件，如果直接运行，可跳过此步骤。如果需要重新转换模型，可参考如下步骤：\n",
    "* **建议在Linux服务器或者虚拟机转换该模型。**\n",
    "* 首先我们可以在[这个链接](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Atlas%20200I%20DK%20A2/DevKit/downloads/23.0.RC1/Ascend-devkit_23.0.RC1_downloads.xlsx)的表格中找到本样例的依赖文件，下载我们已经准备好了的ONNX模型文件，ONNX是开源的离线推理模型框架。\n",
    "\n",
    "* 为了能进一步优化模型推理性能，我们需要将其转换为om模型进行使用，以下为转换指令：  \n",
    "    ```shell\n",
    "    atc --model=offline_encoder_sim.onnx --framework=5 --output=offline_encoder --input_format=ND --input_shape=\"speech:1,1478,80;speech_lengths:1\" --log=error --soc_version=Ascend310B1\n",
    "    ```\n",
    "    其中转换参数的含义为：\n",
    "    * --model：输入模型路径\n",
    "    * --framework：原始网络模型框架类型，5表示ONNX\n",
    "    * --output：输出模型路径\n",
    "    * --input_format：输入Tensor的内存排列方式\n",
    "    * --input_shape：指定模型输入数据的shape。这里我们在转模型的时候指定了最大的输入音频长度，推荐的长度有：262,326,390,454,518,582,646,710,774,838,902,966,1028,1284,1478，默认使用的长度是1478\n",
    "    * --log：日志级别\n",
    "    * --soc_version：昇腾AI处理器型号\n",
    "\n",
    "# 模型推理实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6379716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入代码依赖\n",
    "import torchaudio\n",
    "import torchaudio.compliance.kaldi as kaldi\n",
    "from ais_bench.infer.interface import InferSession\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affedb30-645f-4289-a6f0-c7563d3b1f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeNetASR:\n",
    "    def __init__(self, model_path, vocab_path):\n",
    "        \"\"\"初始化模型，加载词表\"\"\"\n",
    "        self.vocabulary = load_vocab(vocab_path)\n",
    "        self.model = InferSession(0, model_path)\n",
    "        # 获取模型输入特征的最大长度\n",
    "        self.max_len = self.model.get_inputs()[0].shape[1]\n",
    "\n",
    "    def transcribe(self, wav_file):\n",
    "        \"\"\"执行模型推理，将录音文件转为文本。\"\"\"\n",
    "        feats_pad, feats_lengths = self.preprocess(wav_file)\n",
    "        output = self.model.infer([feats_pad, feats_lengths])\n",
    "        txt = self.post_process(output)\n",
    "        return txt\n",
    "\n",
    "    def preprocess(self, wav_file):\n",
    "        \"\"\"数据预处理\"\"\"\n",
    "        waveform, sample_rate = torchaudio.load(wav_file)\n",
    "        # 音频重采样，采样率16000\n",
    "        waveform, sample_rate = resample(waveform, sample_rate, resample_rate=16000)\n",
    "        # 计算fbank特征\n",
    "        feature = compute_fbank(waveform, sample_rate)\n",
    "        feats_lengths = np.array([feature.shape[0]]).astype(np.int32)\n",
    "        # 对输入特征进行padding，使符合模型输入尺寸\n",
    "        feats_pad = pad_sequence(feature,\n",
    "                                 batch_first=True,\n",
    "                                 padding_value=0,\n",
    "                                 max_len=self.max_len)\n",
    "        feats_pad = feats_pad.numpy().astype(np.float32)\n",
    "        return feats_pad, feats_lengths\n",
    "\n",
    "    def post_process(self, output):\n",
    "        \"\"\"对模型推理结果进行后处理，根据贪心策略选择概率最大的token，去除重复字符和空白字符，得到最终文本。\"\"\"\n",
    "        encoder_out_lens, probs_idx = output[1], output[4]\n",
    "        token_idx_list = probs_idx[0, :, 0][:encoder_out_lens[0]]\n",
    "        token_idx_list = remove_duplicates_and_blank(token_idx_list)\n",
    "        text = ''.join(self.vocabulary[token_idx_list])\n",
    "        return text\n",
    "\n",
    "\n",
    "def remove_duplicates_and_blank(token_idx_list):\n",
    "    \"\"\"去除重复字符和空白字符\"\"\"\n",
    "    res = []\n",
    "    cur = 0\n",
    "    BLANK_ID = 0\n",
    "    while cur < len(token_idx_list):\n",
    "        if token_idx_list[cur] != BLANK_ID:\n",
    "            res.append(token_idx_list[cur])\n",
    "        prev = cur\n",
    "        while cur < len(token_idx_list) and token_idx_list[cur] == token_idx_list[prev]:\n",
    "            cur += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def pad_sequence(seq_feature, batch_first=True, padding_value=0, max_len=966):\n",
    "    \"\"\"对输入特征进行padding，使符合模型输入尺寸\"\"\"\n",
    "    feature_shape = seq_feature.shape\n",
    "    feat_len = feature_shape[0]\n",
    "    if feat_len > max_len:\n",
    "        # 如果输入特征长度大于模型输入尺寸，则截断\n",
    "        seq_feature = seq_feature[:max_len].unsqueeze(0)\n",
    "        return seq_feature\n",
    "\n",
    "    batch_size = 1\n",
    "    trailing_dims = feature_shape[1:]\n",
    "    if batch_first:\n",
    "        out_dims = (batch_size, max_len) + trailing_dims\n",
    "    else:\n",
    "        out_dims = (max_len, batch_size) + trailing_dims\n",
    "\n",
    "    out_tensor = seq_feature.data.new(*out_dims).fill_(padding_value)\n",
    "    if batch_first:\n",
    "        out_tensor[0, :feat_len, ...] = seq_feature\n",
    "    else:\n",
    "        out_tensor[:feat_len, 0, ...] = seq_feature\n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "def resample(waveform, sample_rate, resample_rate=16000):\n",
    "    \"\"\"音频重采样\"\"\"\n",
    "    waveform = torchaudio.transforms.Resample(\n",
    "        orig_freq=sample_rate, new_freq=resample_rate)(waveform)\n",
    "    return waveform, resample_rate\n",
    "\n",
    "\n",
    "def compute_fbank(waveform,\n",
    "                  sample_rate,\n",
    "                  num_mel_bins=80,\n",
    "                  frame_length=25,\n",
    "                  frame_shift=10,\n",
    "                  dither=0.0):\n",
    "    \"\"\"提取filter bank音频特征\"\"\"\n",
    "    AMPLIFY_FACTOR = 1 << 15\n",
    "    waveform = waveform * AMPLIFY_FACTOR\n",
    "    mat = kaldi.fbank(waveform,\n",
    "                      num_mel_bins=num_mel_bins,\n",
    "                      frame_length=frame_length,\n",
    "                      frame_shift=frame_shift,\n",
    "                      dither=dither,\n",
    "                      energy_floor=0.0,\n",
    "                      sample_frequency=sample_rate)\n",
    "    return mat\n",
    "\n",
    "\n",
    "def load_vocab(txt_path):\n",
    "    \"\"\"加载词表\"\"\"\n",
    "    vocabulary = []\n",
    "    LEN_OF_VALID_FORMAT = 2\n",
    "    with open(txt_path, 'r') as fin:\n",
    "        for line in fin:\n",
    "            arr = line.strip().split()\n",
    "            # 词表格式：token id\n",
    "            if len(arr) != LEN_OF_VALID_FORMAT:\n",
    "                raise ValueError(f\"Invalid line: {line}. Expect format: token id\")\n",
    "            vocabulary.append(arr[0])\n",
    "    return np.array(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa218b8e",
   "metadata": {},
   "source": [
    "# 样例运行\n",
    "\n",
    "* 初始化相关参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2f6ea-ee0e-435d-a9b8-f1d02e6cbfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"offline_encoder.om\"\n",
    "vocab_path = 'vocab.txt'\n",
    "\n",
    "model = WeNetASR(model_path, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0255970",
   "metadata": {},
   "source": [
    "* 展示样例语音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6fc1b-398a-46fa-aca5-c4d6335bda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file = 'sample.wav'\n",
    "\n",
    "IPython.display.Audio(wav_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de778d00",
   "metadata": {},
   "source": [
    "* 执行模型推理，打印识别文本。从推理结果大家可以看到识别出的文本跟上面的语音内容是一致的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b3c41-7b42-4deb-a73e-d1b739a7fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = model.transcribe(wav_file)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df43cf-936d-4e54-b87b-21229a0a738d",
   "metadata": {},
   "source": [
    "# 样例总结与扩展\n",
    "以上就是这个样例的全部内容了，值得注意的是，由于我们这里用的是静态shape的模型，即模型输入尺寸是固定的，所以在预处理的时候，如果输入的音频长度跟模型输入不同，需要通过padding或截断使其满足模型输入尺寸。大家可以尝试着为该语音识别模型添加下游任务，比如语音控制智能家居等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
