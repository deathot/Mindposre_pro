{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样例介绍 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "传统定义的Optical Character Recognition（光学字符识别）主要完成文档扫描类的工作。\n",
    "\n",
    "如今，OCR一般指Scene Text Recognition （场景文字识别），主要面向自然场景。 OCR两阶段方法一般包含两个模型，检测模型负责找出图像或视频中的文字位置，识别模型负责将图像信息转换为文本信息。\n",
    "\n",
    "此样例中，我们使用的检测模型为CTPN，识别模型则是SVTR。\n",
    "\n",
    "CTPN模型基于Faster RCNN模型修改而来，而SVTR则基于近几年十分流行的Vision Transformer模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前期准备\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 基础镜像的样例目录中已包含转换后的om模型以及测试图片，如果直接运行，可跳过此步骤。如果需要重新转换模型，可以参考下面的步骤。\n",
    "* **建议在Linux服务器或者虚拟机转换该模型。**\n",
    "* 首先我们可以在[这个链接](https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Atlas%20200I%20DK%20A2/DevKit/downloads/23.0.RC1/Ascend-devkit_23.0.RC1_downloads.xlsx)的表格中找到本样例的依赖文件，下载我们已经准备好了的air模型文件与onnx模型文件。air是Mindspore框架的模型结构，而ONNX则是开源的离线推理模型框架。\n",
    "* 为了能进一步优化模型推理性能，我们需要将其转换为om模型进行使用\n",
    "    * 以下为转换指令：\n",
    "        CTPN模型:\n",
    "\n",
    "        ```shell\n",
    "        atc --model=./cnnctc.air --framework=1 --soc_version=Ascend310B1 --output=ctpn --log=info\n",
    "        ```\n",
    "        SVTR模型:\n",
    "        ```shell\n",
    "        atc --model=./svtr.onnx --framework=5 --input_shape='x:1,3,48,1440' --input_format=NCHW --soc_version=Ascend310B1 --output=svtr\n",
    "        ```\n",
    "    * 其中转换参数的含义为：  \n",
    "        * --model：输入模型路径\n",
    "        * --framework：原始网络模型框架类型，1表示air，5表示ONNX\n",
    "        * --output：输出模型路径\n",
    "        * --input_format：输入Tensor的内存排列方式\n",
    "        * --input_shape：指定模型输入数据的shape\n",
    "        * --log：日志级别\n",
    "        * --soc_version：昇腾AI处理器型号"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理实现 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们会简单得介绍一下模型推理的几个必须的步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前处理 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前处理的主要目的是将输入处理成模型需要的输入格式，最常见的比如NLP任务中使用的tokenlizer，CV任务中使用的标准化/色域转换/缩放等操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CTPN ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CTPN模型的前处理由简单的resize和 Normalization组成，代码如下所示\n",
    "\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        # resize image and convert dtype to fp32\n",
    "        dst_img = cv2.resize(img, (int(self.model_width), int(self.model_height))).astype(np.float32)\n",
    "\n",
    "        # normalization\n",
    "        dst_img -= self.mean\n",
    "        dst_img /= self.std\n",
    "\n",
    "        # hwc to chw\n",
    "        dst_img = dst_img.transpose((2, 0, 1))\n",
    "\n",
    "        # chw to nchw\n",
    "        dst_img = np.expand_dims(dst_img, axis=0)\n",
    "        dst_img = np.ascontiguousarray(dst_img).astype(np.float32)\n",
    "        return dst_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVTR ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVTR模型的前处理和CTPN稍有不一样，除了resize和 Normalization之外，为了保证图片缩放后的宽高比以及符合模型输入的shape，我们还需要对其做Padding操作，代码如下所示\n",
    "\n",
    "\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        h, w, _ = img.shape\n",
    "        ratio = w / h\n",
    "        if math.ceil(ratio * self.model_height) > self.model_width:\n",
    "            resize_w = self.model_width\n",
    "        else:\n",
    "            resize_w = math.ceil(ratio * self.model_height)\n",
    "\n",
    "        img = cv2.resize(img, (resize_w, self.model_height))\n",
    "\n",
    "        _, w, _ = img.shape\n",
    "        padding_w = self.model_width - w\n",
    "        img = cv2.copyMakeBorder(img, 0, 0, 0, padding_w, cv2.BORDER_CONSTANT, value=0.).astype(np.float32)\n",
    "        img *= self.scale\n",
    "        img -= self.mean\n",
    "        img /= self.std\n",
    "\n",
    "        # hwc to chw\n",
    "        dst_img = img.transpose((2, 0, 1))\n",
    "\n",
    "        # chw to nchw\n",
    "        dst_img = np.expand_dims(dst_img, axis=0)\n",
    "        dst_img = np.ascontiguousarray(dst_img).astype(np.float32)\n",
    "\n",
    "        return dst_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理接口 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两个模型的推理接口部分是相似的，在复制输入数据至模型输入的地址之后我们调用execute api就可以推理了,代码如下图所示\n",
    "\n",
    "\n",
    "    def infer(self, tensor):\n",
    "        np_ptr = acl.util.bytes_to_ptr(tensor.tobytes())\n",
    "        # copy input data from host to device\n",
    "        ret = acl.rt.memcpy(self.input_data[0][\"buffer\"], self.input_data[0][\"size\"], np_ptr,\n",
    "                            self.input_data[0][\"size\"], ACL_MEMCPY_HOST_TO_DEVICE)\n",
    "\n",
    "        # infer exec\n",
    "        ret = acl.mdl.execute(self.model_id, self.load_input_dataset, self.load_output_dataset)\n",
    "\n",
    "        inference_result = []\n",
    "\n",
    "        for i, item in enumerate(self.output_data):\n",
    "            buffer_host, ret = acl.rt.malloc_host(self.output_data[i][\"size\"])\n",
    "            # copy output data from device to host\n",
    "            ret = acl.rt.memcpy(buffer_host, self.output_data[i][\"size\"], self.output_data[i][\"buffer\"],\n",
    "                                self.output_data[i][\"size\"], ACL_MEMCPY_DEVICE_TO_HOST)\n",
    "\n",
    "            data = acl.util.ptr_to_bytes(buffer_host, self.output_data[i]['size'])\n",
    "            data = np.frombuffer(data, dtype=self.output_dtypes[i]).reshape(self.output_shapes[i])\n",
    "            inference_result.append(data)\n",
    "\n",
    "        return inference_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后处理 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CTPN ####\n",
    "\n",
    "与传统的检测模型的后处理相似，CTPN的后处理也有着NMS等操作。与之不同的是CTPN使用了文本线构造方法使多个bbox组成了一个文本框。代码如下所示\n",
    "\n",
    "    def postprocess(self, output):\n",
    "        proposal = output[0]\n",
    "        proposal_mask = output[1]\n",
    "        all_box_tmp = proposal\n",
    "        all_mask_tmp = np.expand_dims(proposal_mask, axis=1)\n",
    "        using_boxes_mask = all_box_tmp * all_mask_tmp\n",
    "        textsegs = using_boxes_mask[:, 0:4].astype(np.float32)\n",
    "        scores = using_boxes_mask[:, 4].astype(np.float32)\n",
    "        bboxes = detect(textsegs, scores[:, np.newaxis], (self.model_height, self.model_width))\n",
    "        return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVTR ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与大部分文本识别模型相似，SVTR的后处理就是一个简单的Argmax操作之后查表的过程，代码如下所示\n",
    "\n",
    "    def postprocess(self, output):\n",
    "        output = np.argmax(output[0], axis=2).reshape(-1)\n",
    "        ans = []\n",
    "        last_char = ''\n",
    "        for i, char in enumerate(output):\n",
    "            if char and self.labels[char] != last_char:\n",
    "                ans.append(self.labels[char])\n",
    "            last_char = self.labels[char]\n",
    "        return ''.join(ans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 样例运行 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import acl\n",
    "import cv2\n",
    "import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from model import CTPN, SVTR\n",
    "from utils import get_images_from_path, img_read\n",
    "\n",
    "import IPython\n",
    "\n",
    "def init_acl(device_id=0):\n",
    "    acl.init()\n",
    "    ret = acl.rt.set_device(device_id)\n",
    "    if ret:\n",
    "        raise RuntimeError(ret)\n",
    "    context, ret = acl.rt.create_context(device_id)\n",
    "    if ret:\n",
    "        raise RuntimeError(ret)\n",
    "    print('Init ACL Successfully')\n",
    "    return context\n",
    "\n",
    "\n",
    "def deinit_acl(context, device_id=0):\n",
    "    ret = acl.rt.destroy_context(context)\n",
    "    if ret:\n",
    "        raise RuntimeError(ret)\n",
    "\n",
    "    ret = acl.rt.reset_device(device_id)\n",
    "    if ret:\n",
    "        raise RuntimeError(ret)\n",
    "    print('ACL resources were released successfully.')\n",
    "    \n",
    "\n",
    "\n",
    "def get_images_from_path(img_path):\n",
    "    img_list = []\n",
    "    if os.path.isfile(img_path):\n",
    "        img_list.append(img_path)\n",
    "    if os.path.isdir(img_path):\n",
    "        for file in os.listdir(img_path):\n",
    "            img_list.append(os.path.join(img_path, file))\n",
    "    return img_list\n",
    "\n",
    "\n",
    "def img_read(path):\n",
    "    path = os.path.realpath(path)\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"cannot decode file:{path}\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CTPN_MODEL_PATH = './ctpn.om'\n",
    "SVTR_MODEL_PATH = './svtr.om'\n",
    "SVTR_DICT_PATH = './ppocr_keys_v1.txt'\n",
    "DEVICE_ID = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = init_acl(DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model = CTPN(model_path=CTPN_MODEL_PATH, device_id=DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model = SVTR(model_path=SVTR_MODEL_PATH, dict_path=SVTR_DICT_PATH,\n",
    "                     device_id=DEVICE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = './sample.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('infer_result'):\n",
    "    os.makedirs('infer_result')\n",
    "ans = {}\n",
    "\n",
    "img_src = img_read(IMAGE_PATH)\n",
    "basename = os.path.basename(IMAGE_PATH)\n",
    "print(f'start infer image: {basename}')\n",
    "name, ext = os.path.splitext(basename)\n",
    "image_h, image_w = img_src.shape[:2]\n",
    "\n",
    "det_input_tensor = det_model.preprocess(img_src)\n",
    "\n",
    "output = det_model.infer(det_input_tensor)\n",
    "\n",
    "bboxes = det_model.postprocess(output)\n",
    "im = Image.open(IMAGE_PATH)\n",
    "draw = ImageDraw.Draw(im)\n",
    "ans['image_name'] = basename\n",
    "ans['result'] = []\n",
    "for bbox in bboxes:\n",
    "    bbox_detail = {}\n",
    "    x1 = int(bbox[0] / det_model.model_width * image_w)\n",
    "    y1 = int(bbox[1] / det_model.model_height * image_h)\n",
    "    x2 = int(bbox[2] / det_model.model_width * image_w)\n",
    "    y2 = int(bbox[3] / det_model.model_height * image_h)\n",
    "    draw.line([(x1, y1), (x1, y2), (x2, y2), (x2, y1), (x1, y1)], fill='red', width=2)\n",
    "    bbox_detail['bbox'] = [x1, y1, x1, y2, x2, y2, x2, y1]\n",
    "    res  = ','.join(map(str,bbox_detail['bbox']))\n",
    "    print(f'det result: {res}')\n",
    "    crop_img = img_src[y1:y2, x1:x2]\n",
    "\n",
    "    rec_input_tensor = rec_model.preprocess(crop_img)\n",
    "    rec_output = rec_model.infer(rec_input_tensor)\n",
    "    bbox_detail['text'] = rec_model.postprocess(rec_output)\n",
    "    print(f'rec result: {bbox_detail[\"text\"]}')\n",
    "    ans['result'].append(bbox_detail)\n",
    "\n",
    "im.save(os.path.join('infer_result', name + '_res' + ext))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Image(filename=os.path.join('infer_result', name + '_res' + ext)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model.deinit()\n",
    "rec_model.deinit()\n",
    "deinit_acl(context, DEVICE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结与扩展 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是OCR样例的全部内容了，这个流程可以适用于所有两阶段的OCR任务。我们也可以将检测模型换成DBNet或者DBNet++等文本检测模型。或者将识别模型替换为CRNN等文本识别模型。需要注意的是，整个流程最重要的部分是模型前后处理部分的代码，比如Resize的宽高参数，在OCR任务中，前后处理的参数将会对识别结果产生巨大的影响。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
